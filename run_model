#the first 100,000 rows 
data1 = data[1:500000,]

#permutation, split into train and validation dataset
RPerm <- sample(nrow(data1))
data1 <- data1[RPerm,]
TrainInd <- ceiling(nrow(data1)*0.6)
TrainData <- data1[1:TrainInd,]
ValData <- data1[(TrainInd+1):nrow(data1),]

#make matrix
XTrain <- model.matrix(click ~ .,TrainData)
XTrain_dataframe = as.data.frame(XTrain)

XVal <- model.matrix(click ~ .,ValData)
XVal_dataframe = as.data.frame(XVal)

YTrain <- TrainData$click
YVal <- ValData$click

#Random Forest
#The first half
half = 0.5*nrow(data)
firsthalf = data[1:half,]

#500,000 rows per sample
seq.int <- seq(0, nrow(firsthalf), 500000)
seq.gp <- cut(1:nrow(firsthalf), breaks=seq.int, include.lowest=TRUE)
data_list <- split(firsthalf, seq.gp)
#names(data_list) <- paste0("sample_", sprintf("%d", 1:length(data_list)))

Logloss = rep(NA, length(data_list))
for (i in 1:length(data_list)){
  #permutation, split into train and validation dataset
  data1 = as.data.frame(data_list[i])
  RPerm <- sample(nrow(data1))
  data1 <- data1[RPerm,]
  TrainInd <- ceiling(nrow(data1)*0.6)
  TrainData <- data1[1:TrainInd,]
  ValData <- data1[(TrainInd+1):nrow(data1),]
  
  #make matrix
  colnames(TrainData) = colnames(data)
  colnames(ValData) = colnames(data)
  XTrain <- model.matrix(click ~ .,TrainData)
  XTrain_dataframe = as.data.frame(XTrain)
  
  XVal <- model.matrix(click ~ .,ValData)
  XVal_dataframe = as.data.frame(XVal)
  
  YTrain <- TrainData$click
  YVal <- ValData$click
  
  #random forest
  XTrain_dataframe = XTrain_dataframe[,-1]
  XTrain_dataframe$click = YTrain
  RF <- randomForest(click ~ .,data=XTrain_dataframe,
                     sampsize=ceiling(nrow(XTrain_dataframe)/4),
                     ntree=500,maxnodes=50)
  
  ypred <- predict(RF,newdata=XVal)
  LogLoss_RF = logLoss(YVal, ypred)
  Logloss[i] =  LogLoss_RF
  
}

#Logloss
#[1] 0.4356194 0.4194607 0.4330625 0.4461538 0.4851985 0.4869776 0.4534652 0.4744472 0.4245364 0.4416547 0.3974093 0.3953158
#[13] 0.3888248 0.3955234 0.4694267 0.4429351 0.4549144 0.4754439 0.4614317 0.4683891 0.4941327 0.4610517 0.4533197 0.4480675
#[25] 0.4544861 0.4538949 0.4582094 0.4675521 0.4313529 0.4253963 0.4690310
#> mean(Logloss)
#[1] 0.4473124


#the second half
half = 0.5*nrow(data)
secondhalf = data[(half+1):(nrow(data)),]

#500,000 rows per sample
seq.int <- seq(0, nrow(secondhalf), 500000)
seq.gp <- cut(1:nrow(secondhalf), breaks=seq.int, include.lowest=TRUE)
data_list <- split(secondhalf, seq.gp)

Logloss = rep(NA, length(data_list))
for (i in 1:length(data_list)){
  #permutation, split into train and validation dataset
  data1 = as.data.frame(data_list[i])
  RPerm <- sample(nrow(data1))
  data1 <- data1[RPerm,]
  TrainInd <- ceiling(nrow(data1)*0.6)
  TrainData <- data1[1:TrainInd,]
  ValData <- data1[(TrainInd+1):nrow(data1),]
  
  #make matrix
  colnames(TrainData) = colnames(data)
  colnames(ValData) = colnames(data)
  XTrain <- model.matrix(click ~ .,TrainData)
  XTrain_dataframe = as.data.frame(XTrain)
  
  XVal <- model.matrix(click ~ .,ValData)
  XVal_dataframe = as.data.frame(XVal)
  
  YTrain <- TrainData$click
  YVal <- ValData$click
  
  #random forest
  XTrain_dataframe = XTrain_dataframe[,-1]
  XTrain_dataframe$click = YTrain
  RF <- randomForest(click ~ .,data=XTrain_dataframe,
                     sampsize=ceiling(nrow(XTrain_dataframe)/4),
                     mtry=1,ntree=500,maxnodes=50)
  
  ypred <- predict(RF,newdata=XVal)
  LogLoss_RF = logLoss(YVal, ypred)
  Logloss[i] =  LogLoss_RF
  
}


#> Logloss
#[1] 0.4748001 0.4746979 0.4527980 0.4669957 0.4828650 0.4670916 0.4609219 0.4660441 0.4714060 0.4408413 0.4660520 0.4710317
#[13] 0.4497645 0.4821328 0.4533497 0.4384001 0.4423135 0.4471053 0.3717297 0.4069409 0.4126794 0.4009555 0.4371654 0.4089477
#[25] 0.3998878 0.4260116 0.4243101 0.4128340 0.4129315 0.4292844 0.4302865
#> mean(Logloss)
#[1] 0.4413734

#The last performance:
#(0.4413734+0.4473124)/2
#[1] 0.4443429
